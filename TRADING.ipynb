{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Dataset: S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import grid_search\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2000-02-08' 0.01577 0.00949 ... 0.0567 -0.001 0]\n",
      " ['2000-02-09' 0.01594 0.01207 ... 0.0576 -0.0013 -1]\n",
      " ['2000-02-10' -0.00292 -0.007109999999999999 ... 0.0579 0.0 0]\n",
      " ...\n",
      " ['2018-11-07' 0.01938 0.006809999999999999 ... 0.0219 0.0026 1]\n",
      " ['2018-11-08' 0.02278 0.01455 ... 0.0219 0.0026 0]\n",
      " ['2018-11-09' 0.01438 0.0094 ... 0.022000000000000002 0.0025 0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydataset = pd.read_csv('dataset1.csv')\n",
    "spy = mydataset.iloc[:,:].values\n",
    "print(spy)\n",
    "print(type(spy))\n",
    "spy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01577 0.00949 0.00074 ... 0.0225 0.0567 -0.001]\n",
      " [0.01594 0.01207 0.00027 ... 0.0225 0.0576 -0.0013]\n",
      " [-0.00292 -0.007109999999999999 -0.00064 ... 0.0225 0.0579 0.0]\n",
      " ...\n",
      " [0.01938 0.006809999999999999 0.0 ... -0.004 0.0219 0.0026]\n",
      " [0.02278 0.01455 9e-05 ... -0.004 0.0219 0.0026]\n",
      " [0.01438 0.0094 0.0 ... -0.004 0.022000000000000002 0.0025]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=spy[:,1:-1]\n",
    "print(features)\n",
    "features.shape\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 -1 0 ... 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=spy[:,13]\n",
    "print(output)\n",
    "output.shape\n",
    "type(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 train feature shape: (1380, 12)\n",
      "FOLD 1 train output shape: (1380,)\n",
      "FOLD 1 test feature shape: (1380, 12)\n",
      "FOLD 1 test output shape: (1380,)\n",
      "FOLD 2 train feature shape: (2180, 12)\n",
      "FOLD 2 train output shape: (2180,)\n",
      "FOLD 2 test feature shape: (2180, 12)\n",
      "FOLD 2 test output shape: (2180,)\n",
      "FOLD 3 train feature shape: (2980, 12)\n",
      "FOLD 3 train output shape: (2980,)\n",
      "FOLD 3 test feature shape: (2980, 12)\n",
      "FOLD 3 test output shape: (2980,)\n",
      "FOLD 4 train feature shape: (3780, 12)\n",
      "FOLD 4 train output shape: (3780,)\n",
      "FOLD 4 test feature shape: (3780, 12)\n",
      "FOLD 4 test output shape: (3780,)\n"
     ]
    }
   ],
   "source": [
    "#4-FOLD FORWARD CHAINING\n",
    "#70% train, 30% validation\n",
    "#https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "#https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#FOLD 1\n",
    "x1 = features[0:1725,:]\n",
    "y1 = output[0:1725]\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.20, train_size=0.80)\n",
    "x_train1 = x_train1.astype('float')\n",
    "y_train1 = y_train1.astype('int')\n",
    "x_test1 = x_train1.astype('float')\n",
    "y_test1 = y_train1.astype('int')\n",
    "print('FOLD 1 train feature shape:',x_train1.shape)\n",
    "print('FOLD 1 train output shape:',y_train1.shape)\n",
    "print('FOLD 1 test feature shape:',x_test1.shape)\n",
    "print('FOLD 1 test output shape:',y_test1.shape)\n",
    "\n",
    "\n",
    "#FOLD 2\n",
    "x2 = features[0:2725,:]\n",
    "y2 = output[0:2725]\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.20, train_size=0.80)\n",
    "x_train2 = x_train2.astype('float')\n",
    "y_train2 = y_train2.astype('int')\n",
    "x_test2 = x_train2.astype('float')\n",
    "y_test2 = y_train2.astype('int')\n",
    "print('FOLD 2 train feature shape:',x_train2.shape)\n",
    "print('FOLD 2 train output shape:',y_train2.shape)\n",
    "print('FOLD 2 test feature shape:',x_test2.shape)\n",
    "print('FOLD 2 test output shape:',y_test2.shape)\n",
    "\n",
    "\n",
    "#FOLD 3\n",
    "x3 = features[0:3725,:]\n",
    "y3 = output[0:3725]\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(x3, y3, test_size=0.20, train_size=0.80)\n",
    "x_train3 = x_train3.astype('float')\n",
    "y_train3 = y_train3.astype('int')\n",
    "x_test3 = x_train3.astype('float')\n",
    "y_test3 = y_train3.astype('int')\n",
    "print('FOLD 3 train feature shape:',x_train3.shape)\n",
    "print('FOLD 3 train output shape:',y_train3.shape)\n",
    "print('FOLD 3 test feature shape:',x_test3.shape)\n",
    "print('FOLD 3 test output shape:',y_test3.shape)\n",
    "\n",
    "\n",
    "#FOLD 4\n",
    "x4 = features[0:4725,:]\n",
    "y4 = output[0:4725]\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(x4, y4, test_size=0.20, train_size=0.80)\n",
    "x_train4 = x_train4.astype('float')\n",
    "y_train4 = y_train4.astype('int')\n",
    "x_test4 = x_train4.astype('float')\n",
    "y_test4 = y_train4.astype('int')\n",
    "print('FOLD 4 train feature shape:',x_train4.shape)\n",
    "print('FOLD 4 train output shape:',y_train4.shape)\n",
    "print('FOLD 4 test feature shape:',x_test4.shape)\n",
    "print('FOLD 4 test output shape:',y_test4.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random forest 1:  {'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2', 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test accuracy for Random Forest on FOLD 1:  0.6224637681159421\n",
      "Best parameters for Random forest 2:  {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'auto', 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test accuracy for Random Forest on FOLD 2:  0.631651376146789\n",
      "Best parameters for Random forest 3:  {'criterion': 'gini', 'max_depth': 20, 'max_features': 'auto', 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test accuracy for Random Forest on FOLD 3:  0.5956375838926175\n",
      "Best parameters for Random forest 4:  {'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 200}\n",
      "Accuracy for Random Forest on FOLD 4:  0.6476190476190476\n",
      "Average accuracy on test set: 0.624342943943599\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "#FOLD1\n",
    "parameter_rfc1 = {'n_estimators':[10, 200], \n",
    "                   'criterion':('gini', 'entropy'),'max_depth':[1,20], \n",
    "                   'min_samples_split':[2,20], \n",
    "                   'min_samples_leaf':[1,20], \n",
    "                   'max_features':('auto','sqrt','log2'),\n",
    "                   'min_impurity_decrease':[0.0001,0.9999]}\n",
    "\n",
    "\n",
    "CV_rfc1 = GridSearchCV(rfc, parameter_rfc1, cv = 5)\n",
    "CV_rfc1.fit(x_train1, y_train1)\n",
    "print(\"Best parameters for Random forest 1: \",CV_rfc1.best_params_)\n",
    "\n",
    "rfc1 = RandomForestClassifier(criterion = 'gini',\n",
    "                              max_depth = 20,\n",
    "                              max_features = 'auto',\n",
    "                              min_impurity_decrease = 0.0001,\n",
    "                              min_samples_leaf = 20,\n",
    "                              min_samples_split = 20,\n",
    "                              n_estimators = 200)\n",
    "rfc1.fit(x_train1, y_train1)\n",
    "predrfc1 = rfc1.predict(x_test1)\n",
    "print(\"Test accuracy for Random Forest on FOLD 1: \",accuracy_score(y_test1,predrfc1))\n",
    "\n",
    "\n",
    "\n",
    "#FOLD2\n",
    "parameter_rfc2 = {'n_estimators':[10, 200], \n",
    "                   'criterion':('gini', 'entropy'),'max_depth':[1,20], \n",
    "                   'min_samples_split':[2,20], \n",
    "                   'min_samples_leaf':[1,20], \n",
    "                   'max_features':('auto','sqrt','log2'),\n",
    "                   'min_impurity_decrease':[0.0001,0.9999]}\n",
    "\n",
    "\n",
    "CV_rfc2 = GridSearchCV(rfc, parameter_rfc2, cv = 5)\n",
    "CV_rfc2.fit(x_train2, y_train2)\n",
    "print(\"Best parameters for Random forest 2: \",CV_rfc2.best_params_)\n",
    "\n",
    "rfc2 = RandomForestClassifier(criterion = 'gini',\n",
    "                              max_depth = 20,\n",
    "                              max_features = 'auto',\n",
    "                              min_impurity_decrease = 0.0001,\n",
    "                              min_samples_leaf = 20,\n",
    "                              min_samples_split = 2,\n",
    "                              n_estimators = 200)\n",
    "rfc2.fit(x_train2, y_train2)\n",
    "predrfc2 = rfc2.predict(x_test2)\n",
    "print(\"Test accuracy for Random Forest on FOLD 2: \",accuracy_score(y_test2,predrfc2))\n",
    "\n",
    "\n",
    "\n",
    "#FOLD3\n",
    "parameter_rfc3 = {'n_estimators':[10, 200], \n",
    "                   'criterion':('gini', 'entropy'),'max_depth':[1,20], \n",
    "                   'min_samples_split':[2,20], \n",
    "                   'min_samples_leaf':[1,20], \n",
    "                   'max_features':('auto','sqrt','log2'),\n",
    "                   'min_impurity_decrease':[0.0001,0.9999]}\n",
    "\n",
    "\n",
    "CV_rfc3 = GridSearchCV(rfc, parameter_rfc3, cv = 5)\n",
    "CV_rfc3.fit(x_train3, y_train3)\n",
    "print(\"Best parameters for Random forest 3: \",CV_rfc3.best_params_)\n",
    "\n",
    "rfc3 = RandomForestClassifier(criterion = 'gini',\n",
    "                              max_depth = 20,\n",
    "                              max_features = 'auto',\n",
    "                              min_impurity_decrease = 0.0001,\n",
    "                              min_samples_leaf = 20,\n",
    "                              min_samples_split = 2,\n",
    "                              n_estimators = 10)\n",
    "rfc3.fit(x_train3, y_train3)\n",
    "predrfc3 = rfc3.predict(x_test3)\n",
    "print(\"Test accuracy for Random Forest on FOLD 3: \",accuracy_score(y_test3,predrfc3))\n",
    "\n",
    "\n",
    "\n",
    "#FOLD4\n",
    "parameter_rfc4 = {'n_estimators':[10, 200], \n",
    "                   'criterion':('gini', 'entropy'),'max_depth':[1,20], \n",
    "                   'min_samples_split':[2,20], \n",
    "                   'min_samples_leaf':[1,20], \n",
    "                   'max_features':('auto','sqrt','log2'),\n",
    "                   'min_impurity_decrease':[0.0001,0.9999]}\n",
    "\n",
    "CV_rfc4 = GridSearchCV(rfc, parameter_rfc4, cv = 5)\n",
    "CV_rfc4.fit(x_train4, y_train4)\n",
    "print(\"Best parameters for Random forest 4: \",CV_rfc4.best_params_)\n",
    "\n",
    "rfc4 = RandomForestClassifier(criterion = 'entropy',\n",
    "                              max_depth = 20,\n",
    "                              max_features = 'auto',\n",
    "                              min_impurity_decrease = 0.0001,\n",
    "                              min_samples_leaf = 20,\n",
    "                              min_samples_split = 2,\n",
    "                              n_estimators = 200)\n",
    "rfc4.fit(x_train4, y_train4)\n",
    "predrfc4 = rfc4.predict(x_test4)\n",
    "print(\"Accuracy for Random Forest on FOLD 4: \",accuracy_score(y_test4,predrfc4))\n",
    "\n",
    "\n",
    "#AVERAGES\n",
    "score_test_array = np.array([accuracy_score(y_test1,predrfc1),accuracy_score(y_test2,predrfc2), accuracy_score(y_test3,predrfc3), accuracy_score(y_test4,predrfc4)])\n",
    "mean_score_test = np.mean(score_test_array) \n",
    "print('Average accuracy on test set:',mean_score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression 1:  {'C': 10, 'solver': 'sag', 'tol': 0.01}\n",
      "Accuracy for Logistic Regression on FOLD 1:  0.4855072463768116\n",
      "Best parameters for Logistic Regression 2:  {'C': 10, 'solver': 'newton-cg', 'tol': 0.01}\n",
      "Accuracy for Logistic Regression on FOLD 2:  0.47706422018348627\n",
      "Best parameters for Logistic Regression 3:  {'C': 10, 'solver': 'newton-cg', 'tol': 1e-05}\n",
      "Accuracy for Logistic Regression on FOLD 3:  0.5191275167785235\n",
      "Best parameters for Logistic Regression 4:  {'C': 10, 'solver': 'newton-cg', 'tol': 1e-05}\n",
      "Accuracy for Logistic Regression on FOLD 4:  0.5574074074074075\n",
      "Average accuracy on test set: 0.5097765976865571\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "\n",
    "#FOLD1\n",
    "parameter_logistic1 = {'tol':[0.00001, 0.01], \n",
    "                       'C':[0.1, 10], \n",
    "                       'solver':('newton-cg', 'sag', 'lbfgs')}\n",
    "\n",
    "CV_logistic1 = GridSearchCV(logistic, parameter_logistic1, cv = 5)\n",
    "CV_logistic1.fit(x_train1, y_train1)\n",
    "print(\"Best parameters for Logistic Regression 1: \", CV_logistic1.best_params_)\n",
    "\n",
    "logistic1 = LogisticRegression(C = 10, solver = 'sag', tol= 0.01)\n",
    "logistic1.fit(x_train1, y_train1)\n",
    "predlog1 = logistic1.predict(x_test1)\n",
    "print(\"Accuracy for Logistic Regression on FOLD 1: \",accuracy_score(y_test1,predlog1))\n",
    "\n",
    "\n",
    "#FOLD2\n",
    "parameter_logistic2 = {'tol':[0.00001, 0.01], \n",
    "                       'C':[0.1, 10], \n",
    "                       'solver':('newton-cg', 'sag', 'lbfgs')}\n",
    "\n",
    "CV_logistic2 = GridSearchCV(logistic, parameter_logistic2, cv = 5)\n",
    "CV_logistic2.fit(x_train2, y_train2)\n",
    "print(\"Best parameters for Logistic Regression 2: \", CV_logistic2.best_params_)\n",
    "\n",
    "logistic2 = LogisticRegression(C = 10, solver = 'sag', tol = 0.01)\n",
    "logistic2.fit(x_train2, y_train2)\n",
    "predlog2 = logistic2.predict(x_test2)\n",
    "print(\"Accuracy for Logistic Regression on FOLD 2: \",accuracy_score(y_test2,predlog2))\n",
    "\n",
    "#FOLD3\n",
    "parameter_logistic3 = {'tol':[0.00001, 0.01], \n",
    "                       'C':[0.1, 10], \n",
    "                       'solver':('newton-cg', 'sag', 'lbfgs')}\n",
    "\n",
    "CV_logistic3 = GridSearchCV(logistic, parameter_logistic3, cv = 5)\n",
    "CV_logistic3.fit(x_train3, y_train3)\n",
    "print(\"Best parameters for Logistic Regression 3: \", CV_logistic3.best_params_)\n",
    "\n",
    "logistic3 = LogisticRegression(C = 10, solver = 'sag', tol= 0.01)\n",
    "logistic3.fit(x_train3, y_train3)\n",
    "predlog3 = logistic3.predict(x_test3)\n",
    "print(\"Accuracy for Logistic Regression on FOLD 3: \",accuracy_score(y_test3,predlog3))\n",
    "\n",
    "\n",
    "#FOLD4\n",
    "parameter_logistic4 = {'tol':[0.00001, 0.01], \n",
    "                       'C':[0.1, 10], \n",
    "                       'solver':('newton-cg', 'sag', 'lbfgs')}\n",
    "CV_logistic4 = GridSearchCV(logistic, parameter_logistic4, cv = 5)\n",
    "CV_logistic4.fit(x_train4, y_train4)\n",
    "print(\"Best parameters for Logistic Regression 4: \", CV_logistic4.best_params_)\n",
    "\n",
    "logistic4 = LogisticRegression(C = 10, solver = 'sag', tol= 0.01)\n",
    "logistic4.fit(x_train4, y_train4)\n",
    "predlog4 = logistic4.predict(x_test4)\n",
    "print(\"Accuracy for Logistic Regression on FOLD 4: \",accuracy_score(y_test4,predlog4))\n",
    "\n",
    "#AVERAGES\n",
    "score_test_array = np.array([accuracy_score(y_test1,predlog1),accuracy_score(y_test2,predlog2), accuracy_score(y_test3,predlog3), accuracy_score(y_test4,predlog4)])\n",
    "mean_score_test = np.mean(score_test_array) \n",
    "print('Average accuracy on test set:',mean_score_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM1:  {'C': 10, 'gamma': 0.9, 'kernel': 'rbf'}\n",
      "Best score for SVM1:  0.46304347826086956\n",
      "Accuracy for SVM on FOLD 1:  0.46159420289855074\n",
      "Best parameters for SVM2:  {'C': 10, 'kernel': 'linear'}\n",
      "Accuracy for SVM on FOLD 2:  0.4701834862385321\n",
      "Best parameters for SVM3:  {'C': 10, 'kernel': 'linear'}\n",
      "Accuracy for SVM on FOLD 3:  0.5130872483221477\n",
      "Best parameters for SVM4:  {'C': 1, 'kernel': 'linear'}\n",
      "Accuracy for SVM on FOLD 4:  0.5465608465608466\n",
      "Average accuracy on test set: 0.4978564460050193\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "\n",
    "#FOLD1\n",
    "parameter_svm1 = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), \n",
    "                  'C':[0.1, 10],\n",
    "                  'gamma':[0.01,0.9]}\n",
    "\n",
    "CV_svm1 = GridSearchCV(svm, parameter_svm1, cv = 2)\n",
    "CV_svm1.fit(x_train1, y_train1)\n",
    "print(\"Best parameters for SVM1: \", CV_svm1.best_params_)\n",
    "print(\"Best score for SVM1: \", CV_svm1.best_score_)\n",
    "\n",
    "svm1 = SVC(C = 0.1, kernel = 'rbf', gamma = 0.9, decision_function_shape = 'ovo')\n",
    "svm1.fit(x_train1, y_train1)\n",
    "predsvm1 = svm1.predict(x_test1)\n",
    "print(\"Accuracy for SVM on FOLD 1: \",accuracy_score(y_test1,predsvm1))\n",
    "\n",
    "\n",
    "\n",
    "#FOLD2\n",
    "parameter_svm2 = {'kernel':('linear', 'rbf', 'poly'), \n",
    "                  'C':[0.1, 10]}\n",
    "\n",
    "CV_svm2 = GridSearchCV(svm, parameter_svm2, cv = 5)\n",
    "CV_svm2.fit(x_train2, y_train2)\n",
    "print(\"Best parameters for SVM2: \", CV_svm2.best_params_)\n",
    "\n",
    "svm2 = SVC(C = 10, kernel = 'linear')\n",
    "svm2.fit(x_train2, y_train2)\n",
    "predsvm2 = svm2.predict(x_test2)\n",
    "print(\"Accuracy for SVM on FOLD 2: \",accuracy_score(y_test2,predsvm2))\n",
    "\n",
    "#FOLD3\n",
    "parameter_svm3 = {'kernel':('linear', 'rbf', 'poly'), \n",
    "                  'C':[1, 10]}\n",
    "\n",
    "CV_svm3 = GridSearchCV(svm, parameter_svm3, cv = 5)\n",
    "CV_svm3.fit(x_train3, y_train3)\n",
    "print(\"Best parameters for SVM3: \", CV_svm3.best_params_)\n",
    "\n",
    "svm3 = SVC(C = 1, kernel = 'linear')\n",
    "svm3.fit(x_train3, y_train3)\n",
    "predsvm3 = svm2.predict(x_test3)\n",
    "print(\"Accuracy for SVM on FOLD 3: \",accuracy_score(y_test3,predsvm3))\n",
    "\n",
    "#FOLD4\n",
    "parameter_svm4 = {'kernel':('linear', 'rbf', 'poly'), \n",
    "                  'C':[1, 10]}\n",
    "\n",
    "CV_svm4 = GridSearchCV(svm, parameter_svm4, cv = 5)\n",
    "CV_svm4.fit(x_train4, y_train4)\n",
    "print(\"Best parameters for SVM4: \", CV_svm4.best_params_)\n",
    "\n",
    "svm4 = SVC(C = 1, kernel = 'linear')\n",
    "svm4.fit(x_train4, y_train4)\n",
    "predsvm4 = svm4.predict(x_test4)\n",
    "print(\"Accuracy for SVM on FOLD 4: \",accuracy_score(y_test4,predsvm4))\n",
    "\n",
    "\n",
    "#AVERAGES\n",
    "score_test_array = np.array([accuracy_score(y_test1,predsvm1),accuracy_score(y_test2,predsvm2), accuracy_score(y_test3,predsvm3), accuracy_score(y_test4,predsvm4)])\n",
    "mean_score_test = np.mean(score_test_array) \n",
    "print('Average accuracy on test set:',mean_score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#FOLD1\n",
    "#parameter_NN1 = {'hidden_layer_sizes':[1, 10],'activation':('identity', 'relu','logistic', 'tanh'),\n",
    "                # 'solver':('lbfgs', 'sgd', 'adam'), 'alpha':[0.00001,0.01], 'batch_size':[10,100],\n",
    "                #'learning_rate':('constant','invscaling','adaptive'), 'learning_rate_init':[0.00001,0.01],\n",
    "                # 'tol':[0.000001,0.01]}\n",
    "#NN1_train = MLPClassifier()\n",
    "#NN1_train.fit(x_train1, y_train1)\n",
    "#NN1_valid = GridSearchCV(NN1_train, parameter_NN1)\n",
    "#NN1_valid.fit(x_valid1, y_valid1)\n",
    "#SVM1_valid.fit(x_train1, y_train1)\n",
    "#print(NN1_valid.best_params_)\n",
    "#NN1_score_train1 = NN1_train.score(x_train1, y_train1)\n",
    "#NN1_score_valid1 = NN1_valid.score(x_valid1, y_valid1)\n",
    "#NN1_score_test1 = NN1_valid.score(x_test1, y_test1)\n",
    "#print('Accuracy on the training set 1: {:.3f}'.format(NN1_score_train1))\n",
    "#print('Accuracy on the validation set 1: {:.3f}'.format(NN1_score_valid1))\n",
    "#print('Accuracy on the test set 1: {:.3f}'.format(NN1_score_test1))\n",
    "#print(NN1.predict(x_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
